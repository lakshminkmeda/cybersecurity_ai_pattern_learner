{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshminkmeda/Private_GetInTouch_form-handler/blob/master/Second_Try_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS1qKrNa3izu"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=logs\n",
        "\n",
        "#Use Pandas to create a dataframe\n",
        "path = \"/content/drive/My Drive/AI/Dataset/\"\n",
        "files = glob.glob(path + \"/*.csv\")\n",
        "print('File names:', files)\n",
        "dataframe = pd.DataFrame()\n",
        "content = []\n",
        "\n",
        "# To check all the csv files in the path\n",
        "for filename in files:\n",
        "    df = pd.read_csv(filename, index_col=None)\n",
        "    content.append(df)\n",
        "\n",
        "dataframe = pd.concat(content)\n",
        "\n",
        "#print(dataframe.head())\n",
        "#show dataframe details for column types\n",
        "#print(dataframe.info())\n",
        "#print(pd.unique(dataframe['user']))\n",
        "#https://pbpython.com/categorical-encoding.html\n",
        "dataframe[\"time\"] = dataframe[\"time\"].astype('category')\n",
        "dataframe[\"source_address\"] = dataframe[\"source_address\"].astype('category')\n",
        "dataframe[\"destination_address\"] = dataframe[\"destination_address\"].astype('category')\n",
        "dataframe[\"protocol\"] = dataframe[\"protocol\"].astype('category')\n",
        "dataframe[\"label\"] = dataframe[\"label\"].astype('category')\n",
        "dataframe[\"label_detail\"] = dataframe[\"label_detail\"].astype('category')\n",
        "dataframe[\"threat\"] = dataframe[\"threat\"].astype('category')\n",
        "dataframe[\"source_port\"] = dataframe[\"source_port\"].astype('category')\n",
        "dataframe[\"destination_port\"] = dataframe[\"destination_port\"].astype('category')\n",
        "dataframe[\"orig_packets\"] = dataframe[\"orig_packets\"].astype('category')\n",
        "\n",
        "\n",
        "dataframe[\"time_cat\"] = dataframe[\"time\"].cat.codes\n",
        "dataframe[\"source_address_cat\"] = dataframe[\"source_address\"].cat.codes\n",
        "dataframe[\"destination_address_cat\"] = dataframe[\"destination_address\"].cat.codes\n",
        "dataframe[\"protocol_cat\"] = dataframe[\"protocol\"].cat.codes\n",
        "dataframe[\"label_cat\"] = dataframe[\"label\"].cat.codes\n",
        "dataframe[\"label_detail_cat\"] = dataframe[\"label_detail\"].cat.codes\n",
        "dataframe[\"threat_cat\"] = dataframe[\"threat\"].cat.codes\n",
        "dataframe[\"source_port_cat\"] = dataframe[\"source_port\"].cat.codes\n",
        "dataframe[\"destination_port_cat\"] = dataframe[\"destination_port\"].cat.codes\n",
        "dataframe[\"orig_packets_cat\"] = dataframe[\"orig_packets\"].cat.codes\n",
        "#dataframe[\"label_detail\"] = dataframe[\"label_detail\"].astype('string')\n",
        "#print(dataframe.info())\n",
        "#print(dataframe.head())\n",
        "#save dataframe with new columns for future datmapping\n",
        "dataframe.to_csv('dataframe-export-allcolumns.csv')\n",
        "#remove old columns\n",
        "del dataframe[\"time\"]\n",
        "del dataframe[\"source_address\"]\n",
        "del dataframe[\"destination_address\"]\n",
        "del dataframe[\"protocol\"]\n",
        "del dataframe[\"label\"]\n",
        "del dataframe[\"label_detail\"]\n",
        "del dataframe[\"source_port\"]\n",
        "del dataframe[\"destination_port\"]\n",
        "del dataframe[\"threat\"]\n",
        "del dataframe[\"orig_packets\"]\n",
        "#restore original names of columns\n",
        "dataframe.rename(columns={\"source_address_cat\": \"source_address\",\n",
        "                          \"destination_address_cat\": \"destination_address\",\n",
        "                          \"time_cat\": \"time\", \"protocol_cat\": \"protocol\",\n",
        "                          \"label_detail_cat\": \"label_detail\", \"label_cat\":\n",
        "                          \"label\", \"threat_cat\": \"threat\", \"source_port_cat\":\n",
        "                          \"source_port\", \"destination_port_cat\":\n",
        "                          \"destination_port\", \"orig_packets_cat\": \"orig_packets\"\n",
        "                          }, inplace=True)\n",
        "print(dataframe.head())\n",
        "print(dataframe.info())\n",
        "#save dataframe cleaned up\n",
        "dataframe.to_csv('dataframe-export-int-cleaned.csv')\n",
        "\n",
        "#dataframe = np.asarray(dataframe).astype(np.float32)\n",
        "#dataframe = dataframe.astype(np.float32)\n",
        "#tf.convert_to_tensor(dataframe, dtype=tf.int32)\n",
        "#Split the dataframe into train, validation, and test\n",
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')\n",
        "#Create an input pipeline using tf.data\n",
        "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('threat')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds\n",
        "#choose columns needed for calculations (features)\n",
        "feature_columns = []\n",
        "for header in [\"source_address\", \"source_port\", \"protocol\", \"label\", \"label_detail\"]:\n",
        "    feature_columns.append(feature_column.numeric_column(header))\n",
        "#create feature layer\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "#set batch size pipeline\n",
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "#create tensorboard callback\n",
        "tensorboard_callback = [TensorBoard(\n",
        "    log_dir=\"logs\",\n",
        "    histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    write_images=False,\n",
        "    update_freq=\"epoch\",\n",
        ")]\n",
        "\n",
        "#create compile and train model\n",
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_ds,\n",
        "          validation_data=val_ds,\n",
        "          epochs=10, callbacks=[tensorboard_callback])\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(\"Accuracy\", accuracy)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5FAUGs03iz3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgqo7HRW3iz4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
